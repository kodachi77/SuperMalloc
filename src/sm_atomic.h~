#ifndef SM_ATOMIC_H_
#define SM_ATOMIC_H_

// include windows.h or pthreads.h
#if defined( _WIN32 )
#ifndef WIN32_LEAN_AND_MEAN
#define WIN32_LEAN_AND_MEAN
#endif
#include <windows.h>
//#elif !defined( __wasi__ ) && ( !defined( __EMSCRIPTEN__ ) || defined( __EMSCRIPTEN_PTHREADS__ ) )
#else
#define SM_USE_PTHREADS
#include <pthread.h>
#endif

// --------------------------------------------------------------------------------------------
// Atomics
// We need to be portable between C, C++, and MSVC.
// We base the primitives on the C/C++ atomics and create a minimal wrapper for MSVC in C compilation mode.
// This is why we try to use only `uintptr_t` and `<type>*` as atomic types.
// To gain better insight in the range of used atomics, we use explicitly named memory order operations
// instead of passing the memory order as a parameter.
// -----------------------------------------------------------------------------------------------

#if defined( __cplusplus )
// Use C++ atomics
#include <atomic>
#define _Atomic( tp )           std::atomic<tp>
#define sm_atomic( name )       std::atomic_##name
#define sm_memory_order( name ) std::memory_order_##name
#if ( __cplusplus >= 202002L )    // c++20, see issue #571
#define sm_ATOMIC_VAR_INIT( x ) x
#elif !defined( ATOMIC_VAR_INIT )
#define sm_ATOMIC_VAR_INIT( x ) x
#else
#define sm_ATOMIC_VAR_INIT( x ) ATOMIC_VAR_INIT( x )
#endif
//#elif defined(_MSC_VER)
//// Use MSVC C wrapper for C11 atomics
//#define  _Atomic(tp)              tp
//#define  sm_ATOMIC_VAR_INIT(x)    x
//#define  sm_atomic(name)          sm_atomic_##name
//#define  sm_memory_order(name)    sm_memory_order_##name
#else
// Use C11 atomics
#include <stdatomic.h>
#define sm_atomic( name )       atomic_##name
#define sm_memory_order( name ) memory_order_##name
#if ( __STDC_VERSION__ >= 201710L )    // c17, see issue #735
#define sm_ATOMIC_VAR_INIT( x ) x
#elif !defined( ATOMIC_VAR_INIT )
#define sm_ATOMIC_VAR_INIT( x ) x
#else
#define sm_ATOMIC_VAR_INIT( x ) ATOMIC_VAR_INIT( x )
#endif
#endif

//typedef uintptr_t sm_atomic_ptr;

// Various defines for all used memory orders in SuperMalloc
#define sm_atomic_cas_weak( p, expected, desired, mem_success, mem_fail )                                                        \
    sm_atomic( compare_exchange_weak_explicit )( p, expected, desired, mem_success, mem_fail )

#define sm_atomic_cas_strong( p, expected, desired, mem_success, mem_fail )                                                      \
    sm_atomic( compare_exchange_strong_explicit )( p, expected, desired, mem_success, mem_fail )

#define sm_atomic_load_acquire( p )        sm_atomic( load_explicit )( p, sm_memory_order( acquire ) )
#define sm_atomic_load_relaxed( p )        sm_atomic( load_explicit )( p, sm_memory_order( relaxed ) )
#define sm_atomic_store_release( p, x )    sm_atomic( store_explicit )( p, x, sm_memory_order( release ) )
#define sm_atomic_store_relaxed( p, x )    sm_atomic( store_explicit )( p, x, sm_memory_order( relaxed ) )
#define sm_atomic_exchange_relaxed( p, x ) sm_atomic( exchange_explicit )( p, x, sm_memory_order( relaxed ) )
#define sm_atomic_exchange_release( p, x ) sm_atomic( exchange_explicit )( p, x, sm_memory_order( release ) )
#define sm_atomic_exchange_acq_rel( p, x ) sm_atomic( exchange_explicit )( p, x, sm_memory_order( acq_rel ) )
#define sm_atomic_cas_weak_release( p, exp, des )                                                                                \
    sm_atomic_cas_weak( p, exp, des, sm_memory_order( release ), sm_memory_order( relaxed ) )
#define sm_atomic_cas_weak_acq_rel( p, exp, des )                                                                                \
    sm_atomic_cas_weak( p, exp, des, sm_memory_order( acq_rel ), sm_memory_order( acquire ) )
#define sm_atomic_cas_strong_release( p, exp, des )                                                                              \
    sm_atomic_cas_strong( p, exp, des, sm_memory_order( release ), sm_memory_order( relaxed ) )
#define sm_atomic_cas_strong_acq_rel( p, exp, des )                                                                              \
    sm_atomic_cas_strong( p, exp, des, sm_memory_order( acq_rel ), sm_memory_order( acquire ) )

#define sm_atomic_add_relaxed( p, x ) sm_atomic( fetch_add_explicit )( p, x, sm_memory_order( relaxed ) )
#define sm_atomic_sub_relaxed( p, x ) sm_atomic( fetch_sub_explicit )( p, x, sm_memory_order( relaxed ) )
#define sm_atomic_add_acq_rel( p, x ) sm_atomic( fetch_add_explicit )( p, x, sm_memory_order( acq_rel ) )
#define sm_atomic_sub_acq_rel( p, x ) sm_atomic( fetch_sub_explicit )( p, x, sm_memory_order( acq_rel ) )
#define sm_atomic_and_acq_rel( p, x ) sm_atomic( fetch_and_explicit )( p, x, sm_memory_order( acq_rel ) )
#define sm_atomic_or_acq_rel( p, x )  sm_atomic( fetch_or_explicit )( p, x, sm_memory_order( acq_rel ) )

#define sm_atomic_increment_relaxed( p ) sm_atomic_add_relaxed( p, (uintptr_t) 1 )
#define sm_atomic_decrement_relaxed( p ) sm_atomic_sub_relaxed( p, (uintptr_t) 1 )
#define sm_atomic_increment_acq_rel( p ) sm_atomic_add_acq_rel( p, (uintptr_t) 1 )
#define sm_atomic_decrement_acq_rel( p ) sm_atomic_sub_acq_rel( p, (uintptr_t) 1 )

static inline void     sm_atomic_yield( void );
static inline intptr_t sm_atomic_addi( _Atomic( intptr_t )* p, intptr_t add );
static inline intptr_t sm_atomic_subi( _Atomic( intptr_t )* p, intptr_t sub );

#if defined( __cplusplus )    //>|| !defined(_MSC_VER)

// In C++/C11 atomics we have polymorphic atomics so can use the typed `ptr` variants (where `tp` is the type of atomic value)
// We use these macros so we can provide a typed wrapper in MSVC in C compilation mode as well
#define sm_atomic_load_ptr_acquire( tp, p ) sm_atomic_load_acquire( p )
#define sm_atomic_load_ptr_relaxed( tp, p ) sm_atomic_load_relaxed( p )

// In C++ we need to add casts to help resolve templates if NULL is passed
#if defined( __cplusplus )
#define sm_atomic_store_ptr_release( tp, p, x )             sm_atomic_store_release( p, (tp*) x )
#define sm_atomic_store_ptr_relaxed( tp, p, x )             sm_atomic_store_relaxed( p, (tp*) x )
#define sm_atomic_cas_ptr_weak_release( tp, p, exp, des )   sm_atomic_cas_weak_release( p, exp, (tp*) des )
#define sm_atomic_cas_ptr_weak_acq_rel( tp, p, exp, des )   sm_atomic_cas_weak_acq_rel( p, exp, (tp*) des )
#define sm_atomic_cas_ptr_strong_release( tp, p, exp, des ) sm_atomic_cas_strong_release( p, exp, (tp*) des )
#define sm_atomic_cas_ptr_strong_acq_rel( tp, p, exp, des ) sm_atomic_cas_strong_acq_rel( p, exp, (tp*) des )
#define sm_atomic_exchange_ptr_relaxed( tp, p, x )          sm_atomic_exchange_relaxed( p, (tp*) x )
#define sm_atomic_exchange_ptr_release( tp, p, x )          sm_atomic_exchange_release( p, (tp*) x )
#define sm_atomic_exchange_ptr_acq_rel( tp, p, x )          sm_atomic_exchange_acq_rel( p, (tp*) x )
#else
#define sm_atomic_store_ptr_release( tp, p, x )             sm_atomic_store_release( p, x )
#define sm_atomic_store_ptr_relaxed( tp, p, x )             sm_atomic_store_relaxed( p, x )
#define sm_atomic_cas_ptr_weak_release( tp, p, exp, des )   sm_atomic_cas_weak_release( p, exp, des )
#define sm_atomic_cas_ptr_weak_acq_rel( tp, p, exp, des )   sm_atomic_cas_weak_acq_rel( p, exp, des )
#define sm_atomic_cas_ptr_strong_release( tp, p, exp, des ) sm_atomic_cas_strong_release( p, exp, des )
#define sm_atomic_cas_ptr_strong_acq_rel( tp, p, exp, des ) sm_atomic_cas_strong_acq_rel( p, exp, des )
#define sm_atomic_exchange_ptr_relaxed( tp, p, x )          sm_atomic_exchange_relaxed( p, x )
#define sm_atomic_exchange_ptr_release( tp, p, x )          sm_atomic_exchange_release( p, x )
#define sm_atomic_exchange_ptr_acq_rel( tp, p, x )          sm_atomic_exchange_acq_rel( p, x )
#endif

#define sm_atomic_test_and_set( ptr ) sm_atomic_cas_strong_acq_rel( ptr, (uintptr_t*) 1, (uintptr_t) 1 )

//_atomic_exchange_f( ( dst ) )( ( dst ), 1, memory_order_seq_cst )
//#define atomic_flag_test_and_set( ptr ) _atomic_uint8_exchange_explicit( ( ptr ), 1, memory_order_seq_cst )

// // These are used by the statistics
// static inline int64_t sm_atomic_addi64_relaxed(volatile int64_t* p, int64_t add) {
//   return sm_atomic(fetch_add_explicit)((_Atomic(int64_t)*)p, add, sm_memory_order(relaxed));
// }
// static inline void sm_atomic_void_addi64_relaxed(volatile int64_t* p, const volatile int64_t* padd) {
//   const int64_t add = sm_atomic_load_relaxed((_Atomic(int64_t)*)padd);
//   if (add != 0) {
//     sm_atomic(fetch_add_explicit)((_Atomic(int64_t)*)p, add, sm_memory_order(relaxed));
//   }
// }
// static inline void sm_atomic_maxi64_relaxed(volatile int64_t* p, int64_t x) {
//   int64_t current = sm_atomic_load_relaxed((_Atomic(int64_t)*)p);
//   while (current < x && !sm_atomic_cas_weak_release((_Atomic(int64_t)*)p, &current, x)) { /* nothing */ };
// }

// // Used by timers
// #define sm_atomic_loadi64_acquire(p)            sm_atomic(load_explicit)(p,sm_memory_order(acquire))
// #define sm_atomic_loadi64_relaxed(p)            sm_atomic(load_explicit)(p,sm_memory_order(relaxed))
// #define sm_atomic_storei64_release(p,x)         sm_atomic(store_explicit)(p,x,sm_memory_order(release))
// #define sm_atomic_storei64_relaxed(p,x)         sm_atomic(store_explicit)(p,x,sm_memory_order(relaxed))

// #define sm_atomic_casi64_strong_acq_rel(p,e,d)  sm_atomic_cas_strong_acq_rel(p,e,d)
// #define sm_atomic_addi64_acq_rel(p,i)           sm_atomic_add_acq_rel(p,i)

// #elif defined(_MSC_VER)

// // Legacy MSVC plain C compilation wrapper that uses Interlocked operations to model C11 atomics.
// #include <intrin.h>
// #ifdef _WIN64
// typedef LONG64   msc_intptr_t;
// #define sm_64(f) f##64
// #else
// typedef LONG     msc_intptr_t;
// #define sm_64(f) f
// #endif

// typedef enum sm_memory_order_e {
//   sm_memory_order_relaxed,
//   sm_memory_order_consume,
//   sm_memory_order_acquire,
//   sm_memory_order_release,
//   sm_memory_order_acq_rel,
//   sm_memory_order_seq_cst
// } sm_memory_order;

// static inline uintptr_t sm_atomic_fetch_add_explicit(_Atomic(uintptr_t)*p, uintptr_t add, sm_memory_order mo) {
//   (void)(mo);
//   return (uintptr_t)sm_64(_InterlockedExchangeAdd)((volatile msc_intptr_t*)p, (msc_intptr_t)add);
// }
// static inline uintptr_t sm_atomic_fetch_sub_explicit(_Atomic(uintptr_t)*p, uintptr_t sub, sm_memory_order mo) {
//   (void)(mo);
//   return (uintptr_t)sm_64(_InterlockedExchangeAdd)((volatile msc_intptr_t*)p, -((msc_intptr_t)sub));
// }
// static inline uintptr_t sm_atomic_fetch_and_explicit(_Atomic(uintptr_t)*p, uintptr_t x, sm_memory_order mo) {
//   (void)(mo);
//   return (uintptr_t)sm_64(_InterlockedAnd)((volatile msc_intptr_t*)p, (msc_intptr_t)x);
// }
// static inline uintptr_t sm_atomic_fetch_or_explicit(_Atomic(uintptr_t)*p, uintptr_t x, sm_memory_order mo) {
//   (void)(mo);
//   return (uintptr_t)sm_64(_InterlockedOr)((volatile msc_intptr_t*)p, (msc_intptr_t)x);
// }
// static inline bool sm_atomic_compare_exchange_strong_explicit(_Atomic(uintptr_t)*p, uintptr_t* expected, uintptr_t desired, sm_memory_order mo1, sm_memory_order mo2) {
//   (void)(mo1); (void)(mo2);
//   uintptr_t read = (uintptr_t)sm_64(_InterlockedCompareExchange)((volatile msc_intptr_t*)p, (msc_intptr_t)desired, (msc_intptr_t)(*expected));
//   if (read == *expected) {
//     return true;
//   }
//   else {
//     *expected = read;
//     return false;
//   }
// }
// static inline bool sm_atomic_compare_exchange_weak_explicit(_Atomic(uintptr_t)*p, uintptr_t* expected, uintptr_t desired, sm_memory_order mo1, sm_memory_order mo2) {
//   return sm_atomic_compare_exchange_strong_explicit(p, expected, desired, mo1, mo2);
// }
// static inline uintptr_t sm_atomic_exchange_explicit(_Atomic(uintptr_t)*p, uintptr_t exchange, sm_memory_order mo) {
//   (void)(mo);
//   return (uintptr_t)sm_64(_InterlockedExchange)((volatile msc_intptr_t*)p, (msc_intptr_t)exchange);
// }
// static inline void sm_atomic_thread_fence(sm_memory_order mo) {
//   (void)(mo);
//   _Atomic(uintptr_t) x = 0;
//   sm_atomic_exchange_explicit(&x, 1, mo);
// }
// static inline uintptr_t sm_atomic_load_explicit(_Atomic(uintptr_t) const* p, sm_memory_order mo) {
//   (void)(mo);
// #if defined(_M_IX86) || defined(_M_X64)
//   return *p;
// #else
//   uintptr_t x = *p;
//   if (mo > sm_memory_order_relaxed) {
//     while (!sm_atomic_compare_exchange_weak_explicit((_Atomic(uintptr_t)*)p, &x, x, mo, sm_memory_order_relaxed)) { /* nothing */ };
//   }
//   return x;
// #endif
// }
// static inline void sm_atomic_store_explicit(_Atomic(uintptr_t)*p, uintptr_t x, sm_memory_order mo) {
//   (void)(mo);
// #if defined(_M_IX86) || defined(_M_X64)
//   *p = x;
// #else
//   sm_atomic_exchange_explicit(p, x, mo);
// #endif
// }
// static inline int64_t sm_atomic_loadi64_explicit(_Atomic(int64_t)*p, sm_memory_order mo) {
//   (void)(mo);
// #if defined(_M_X64)
//   return *p;
// #else
//   int64_t old = *p;
//   int64_t x = old;
//   while ((old = InterlockedCompareExchange64(p, x, old)) != x) {
//     x = old;
//   }
//   return x;
// #endif
// }
// static inline void sm_atomic_storei64_explicit(_Atomic(int64_t)*p, int64_t x, sm_memory_order mo) {
//   (void)(mo);
// #if defined(x_M_IX86) || defined(_M_X64)
//   *p = x;
// #else
//   InterlockedExchange64(p, x);
// #endif
// }

// // These are used by the statistics
// static inline int64_t sm_atomic_addi64_relaxed(volatile _Atomic(int64_t)*p, int64_t add) {
// #ifdef _WIN64
//   return (int64_t)sm_atomic_addi((int64_t*)p, add);
// #else
//   int64_t current;
//   int64_t sum;
//   do {
//     current = *p;
//     sum = current + add;
//   } while (_InterlockedCompareExchange64(p, sum, current) != current);
//   return current;
// #endif
// }
// static inline void sm_atomic_void_addi64_relaxed(volatile int64_t* p, const volatile int64_t* padd) {
//   const int64_t add = *padd;
//   if (add != 0) {
//     sm_atomic_addi64_relaxed((volatile _Atomic(int64_t)*)p, add);
//   }
// }

// static inline void sm_atomic_maxi64_relaxed(volatile _Atomic(int64_t)*p, int64_t x) {
//   int64_t current;
//   do {
//     current = *p;
//   } while (current < x && _InterlockedCompareExchange64(p, x, current) != current);
// }

// static inline void sm_atomic_addi64_acq_rel(volatile _Atomic(int64_t*)p, int64_t i) {
//   sm_atomic_addi64_relaxed(p, i);
// }

// static inline bool sm_atomic_casi64_strong_acq_rel(volatile _Atomic(int64_t*)p, int64_t* exp, int64_t des) {
//   int64_t read = _InterlockedCompareExchange64(p, des, *exp);
//   if (read == *exp) {
//     return true;
//   }
//   else {
//     *exp = read;
//     return false;
//   }
// }

// // The pointer macros cast to `uintptr_t`.
// #define sm_atomic_load_ptr_acquire(tp,p)                (tp*)sm_atomic_load_acquire((_Atomic(uintptr_t)*)(p))
// #define sm_atomic_load_ptr_relaxed(tp,p)                (tp*)sm_atomic_load_relaxed((_Atomic(uintptr_t)*)(p))
// #define sm_atomic_store_ptr_release(tp,p,x)             sm_atomic_store_release((_Atomic(uintptr_t)*)(p),(uintptr_t)(x))
// #define sm_atomic_store_ptr_relaxed(tp,p,x)             sm_atomic_store_relaxed((_Atomic(uintptr_t)*)(p),(uintptr_t)(x))
// #define sm_atomic_cas_ptr_weak_release(tp,p,exp,des)    sm_atomic_cas_weak_release((_Atomic(uintptr_t)*)(p),(uintptr_t*)exp,(uintptr_t)des)
// #define sm_atomic_cas_ptr_weak_acq_rel(tp,p,exp,des)    sm_atomic_cas_weak_acq_rel((_Atomic(uintptr_t)*)(p),(uintptr_t*)exp,(uintptr_t)des)
// #define sm_atomic_cas_ptr_strong_release(tp,p,exp,des)  sm_atomic_cas_strong_release((_Atomic(uintptr_t)*)(p),(uintptr_t*)exp,(uintptr_t)des)
// #define sm_atomic_cas_ptr_strong_acq_rel(tp,p,exp,des)  sm_atomic_cas_strong_acq_rel((_Atomic(uintptr_t)*)(p),(uintptr_t*)exp,(uintptr_t)des)
// #define sm_atomic_exchange_ptr_relaxed(tp,p,x)          (tp*)sm_atomic_exchange_relaxed((_Atomic(uintptr_t)*)(p),(uintptr_t)x)
// #define sm_atomic_exchange_ptr_release(tp,p,x)          (tp*)sm_atomic_exchange_release((_Atomic(uintptr_t)*)(p),(uintptr_t)x)
// #define sm_atomic_exchange_ptr_acq_rel(tp,p,x)          (tp*)sm_atomic_exchange_acq_rel((_Atomic(uintptr_t)*)(p),(uintptr_t)x)

// #define sm_atomic_loadi64_acquire(p)    sm_atomic(loadi64_explicit)(p,sm_memory_order(acquire))
// #define sm_atomic_loadi64_relaxed(p)    sm_atomic(loadi64_explicit)(p,sm_memory_order(relaxed))
// #define sm_atomic_storei64_release(p,x) sm_atomic(storei64_explicit)(p,x,sm_memory_order(release))
// #define sm_atomic_storei64_relaxed(p,x) sm_atomic(storei64_explicit)(p,x,sm_memory_order(relaxed))

#endif

// // Atomically add a signed value; returns the previous value.
// static inline intptr_t sm_atomic_addi(_Atomic(intptr_t)*p, intptr_t add) {
//   return (intptr_t)sm_atomic_add_acq_rel((_Atomic(uintptr_t)*)p, (uintptr_t)add);
// }

// // Atomically subtract a signed value; returns the previous value.
// static inline intptr_t sm_atomic_subi(_Atomic(intptr_t)*p, intptr_t sub) {
//   return (intptr_t)sm_atomic_addi(p, -sub);
// }

// ----------------------------------------------------------------------
// Once and Guard
// ----------------------------------------------------------------------

typedef _Atomic( uintptr_t ) sm_atomic_once_t;

// Returns true only on the first invocation
static inline bool
sm_atomic_once( sm_atomic_once_t* once )
{
    if( sm_atomic_load_relaxed( once ) != 0 ) return false;    // quick test
    uintptr_t expected = 0;
    return sm_atomic_cas_strong_acq_rel( once, &expected, (uintptr_t) 1 );    // try to set to 1
}

typedef _Atomic( uintptr_t ) sm_atomic_guard_t;

// Allows only one thread to execute at a time
#define sm_atomic_guard( guard )                                                                                                 \
    uintptr_t _sm_guard_expected = 0;                                                                                            \
    for( bool _sm_guard_once = true;                                                                                             \
         _sm_guard_once && sm_atomic_cas_strong_acq_rel( guard, &_sm_guard_expected, (uintptr_t) 1 );                            \
         ( sm_atomic_store_release( guard, (uintptr_t) 0 ), _sm_guard_once = false ) )

// ----------------------------------------------------------------------
// Yield
// ----------------------------------------------------------------------

#if defined( __cplusplus )
#include <thread>
static inline void
sm_atomic_yield( void )
{
    std::this_thread::yield();
}
#elif defined( _WIN32 )
static inline void
sm_atomic_yield( void )
{
    YieldProcessor();
}
#elif defined( __SSE2__ )
#include <emmintrin.h>
static inline void
sm_atomic_yield( void )
{
    _mm_pause();
}
#elif ( defined( __GNUC__ ) || defined( __clang__ ) )                                                                            \
    && ( defined( __x86_64__ ) || defined( __i386__ ) || defined( __aarch64__ ) || defined( __arm__ ) || defined( __powerpc__ )  \
         || defined( __ppc__ ) || defined( __PPC__ ) || defined( __POWERPC__ ) )
#if defined( __x86_64__ ) || defined( __i386__ )
static inline void
sm_atomic_yield( void )
{
    __asm__ volatile( "pause" ::: "memory" );
}
#elif defined( __aarch64__ )
static inline void
sm_atomic_yield( void )
{
    __asm__ volatile( "wfe" );
}
#elif defined( __arm__ )
#if __ARM_ARCH >= 7
static inline void
sm_atomic_yield( void )
{
    __asm__ volatile( "yield" ::: "memory" );
}
#else
static inline void
sm_atomic_yield( void )
{
    __asm__ volatile( "nop" ::: "memory" );
}
#endif
#elif defined( __powerpc__ ) || defined( __ppc__ ) || defined( __PPC__ ) || defined( __POWERPC__ )
#ifdef __APPLE__
static inline void
sm_atomic_yield( void )
{
    __asm__ volatile( "or r27,r27,r27" ::: "memory" );
}
#else
static inline void
sm_atomic_yield( void )
{
    __asm__ __volatile__( "or 27,27,27" ::: "memory" );
}
#endif
#endif
#elif defined( __sun )
// Fallback for other archs
#include <synch.h>
static inline void
sm_atomic_yield( void )
{
    smt_pause();
}
#elif defined( __wasi__ )
#include <sched.h>
static inline void
sm_atomic_yield( void )
{
    sched_yield();
}
#else
#include <unistd.h>
static inline void
sm_atomic_yield( void )
{
    sleep( 0 );
}
#endif

// ----------------------------------------------------------------------
// Locks
// These do not have to be recursive and should be light-weight
// in-process only locks. Only used for reserving arena's and to
// maintain the abandoned list.
// ----------------------------------------------------------------------
#if _MSC_VER
#pragma warning( disable : 26110 )    // unlock with holding lock
#endif

#define sm_lock( lock ) for( bool _go = ( sm_lock_acquire( lock ), true ); _go; ( sm_lock_release( lock ), _go = false ) )

#if defined( _WIN32 )

#if 1
#define sm_lock_t SRWLOCK    // slim reader-writer lock

static inline bool
sm_lock_try_acquire( sm_lock_t* lock )
{
    return TryAcquireSRWLockExclusive( lock );
}
static inline void
sm_lock_acquire( sm_lock_t* lock )
{
    AcquireSRWLockExclusive( lock );
}
static inline void
sm_lock_release( sm_lock_t* lock )
{
    ReleaseSRWLockExclusive( lock );
}
static inline void
sm_lock_init( sm_lock_t* lock )
{
    InitializeSRWLock( lock );
}
static inline void
sm_lock_done( sm_lock_t* lock )
{
    (void) ( lock );
}

#else
#define sm_lock_t CRITICAL_SECTION

static inline bool
sm_lock_try_acquire( sm_lock_t* lock )
{
    return TryEnterCriticalSection( lock );
}
static inline void
sm_lock_acquire( sm_lock_t* lock )
{
    EnterCriticalSection( lock );
}
static inline void
sm_lock_release( sm_lock_t* lock )
{
    LeaveCriticalSection( lock );
}
static inline void
sm_lock_init( sm_lock_t* lock )
{
    InitializeCriticalSection( lock );
}
static inline void
sm_lock_done( sm_lock_t* lock )
{
    DeleteCriticalSection( lock );
}

#endif

#elif defined( SM_USE_PTHREADS )

void _sm_error_message( int err, const char* fmt, ... );

#define sm_lock_t pthread_mutex_t

static inline bool
sm_lock_try_acquire( sm_lock_t* lock )
{
    return ( pthread_mutex_trylock( lock ) == 0 );
}
static inline void
sm_lock_acquire( sm_lock_t* lock )
{
    const int err = pthread_mutex_lock( lock );
    if( err != 0 ) { _sm_error_message( err, "internal error: lock cannot be acquired\n" ); }
}
static inline void
sm_lock_release( sm_lock_t* lock )
{
    pthread_mutex_unlock( lock );
}
static inline void
sm_lock_init( sm_lock_t* lock )
{
    pthread_mutex_init( lock, NULL );
}
static inline void
sm_lock_done( sm_lock_t* lock )
{
    pthread_mutex_destroy( lock );
}

#elif defined( __cplusplus )

#include <mutex>
#define sm_lock_t std::mutex

static inline bool
sm_lock_try_acquire( sm_lock_t* lock )
{
    return lock->try_lock();
}
static inline void
sm_lock_acquire( sm_lock_t* lock )
{
    lock->lock();
}
static inline void
sm_lock_release( sm_lock_t* lock )
{
    lock->unlock();
}
static inline void
sm_lock_init( sm_lock_t* lock )
{
    (void) ( lock );
}
static inline void
sm_lock_done( sm_lock_t* lock )
{
    (void) ( lock );
}

// #else

// // fall back to poor man's locks.
// // this should only be the case in a single-threaded environment (like __wasi__)

// #define sm_lock_t  _Atomic(uintptr_t)

// static inline bool sm_lock_try_acquire(sm_lock_t* lock) {
//   uintptr_t expected = 0;
//   return sm_atomic_cas_strong_acq_rel(lock, &expected, (uintptr_t)1);
// }
// static inline void sm_lock_acquire(sm_lock_t* lock) {
//   for (int i = 0; i < 1000; i++) {  // for at most 1000 tries?
//     if (sm_lock_try_acquire(lock)) return;
//     sm_atomic_yield();
//   }
// }
// static inline void sm_lock_release(sm_lock_t* lock) {
//   sm_atomic_store_release(lock, (uintptr_t)0);
// }
// static inline void sm_lock_init(sm_lock_t* lock) {
//   sm_lock_release(lock);
// }
// static inline void sm_lock_done(sm_lock_t* lock) {
//   (void)(lock);
// }

#endif

#endif /* SM_ATOMIC_H_ */
